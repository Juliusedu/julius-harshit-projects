{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bBKPqgDTwgyqMyNNp5YY9hw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 11428,
          "status": "ok",
          "timestamp": 1726515042450,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "3bBKPqgDTwgyqMyNNp5YY9hw",
        "outputId": "64624127-7b99-4d6b-ca64-0a2f98c591ff",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install -q google-colab-selenium\n",
        "%pip install -q selenium"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RLEhvzHcntsz",
      "metadata": {
        "id": "RLEhvzHcntsz"
      },
      "source": [
        "Code to scrape from apprentice job data and insert one row at a time into bigquery and be able to view in google sheets (Apprenticeship Data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LcPnd2cyGfb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "LcPnd2cyGfb4",
        "outputId": "2a4a2560-1f5b-435b-b90b-0c8ce93e2bd0"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from datetime import datetime\n",
        "import google_colab_selenium as gs\n",
        "\n",
        "def scrape_jobs(occupation, project_id, dataset_id, table_id):\n",
        "    # Initialize WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode\n",
        "    options.add_argument('--disable-blink-features=AutomationControlled')  # Make Selenium less detectable\n",
        "    driver = gs.Chrome(options=options)\n",
        "\n",
        "    url = f\"https://www.apprenticeship.gov/finder/listings?occupation={occupation.replace(' ', '%20')}&location=\"\n",
        "    driver.get(url)\n",
        "\n",
        "    try:\n",
        "        # Load More button logic\n",
        "        while True:\n",
        "            try:\n",
        "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # Scroll to the bottom of the page\n",
        "                load_more_button = WebDriverWait(driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \".listings-cards-load-more-btn-container .btn.btn--secondary.load-more\"))\n",
        "                )\n",
        "                load_more_button.click()\n",
        "                time.sleep(5)  # Wait for new jobs to load\n",
        "            except Exception as e:\n",
        "                print(f\"Load more button not found or not clickable: {e}\")\n",
        "                break\n",
        "\n",
        "        # Wait for job listings to load\n",
        "        jobs = WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]'))\n",
        "        )\n",
        "\n",
        "        # Iterate over each job listing\n",
        "        for i in range(len(jobs)):\n",
        "            try:\n",
        "                # Re-find job elements to avoid stale element reference\n",
        "                jobs = driver.find_elements(By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]')\n",
        "                job = jobs[i]\n",
        "                time.sleep(5)\n",
        "\n",
        "                # Extract title, program, and location\n",
        "                title = job.find_element(By.CLASS_NAME, 'left-section_main-title').text\n",
        "                program = job.find_element(By.CLASS_NAME, 'bottom-content_p-title').text\n",
        "                location = job.find_element(By.CLASS_NAME, 'bottom-content_p-content').text\n",
        "\n",
        "                # Click the job to open the details view\n",
        "                job.click()\n",
        "\n",
        "                # Wait for the job summary to load\n",
        "                summary_element = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CLASS_NAME, \"description-section\"))\n",
        "                )\n",
        "                summary = summary_element.text\n",
        "\n",
        "                # Create a job dictionary\n",
        "                job_listing = {\n",
        "                    \"job_title\": title,\n",
        "                    \"company\": program,\n",
        "                    \"location\": location,\n",
        "                    \"summary\": summary,\n",
        "                    \"scraped_occupation_title\": occupation\n",
        "                }\n",
        "\n",
        "                # Immediately upload the job to BigQuery\n",
        "                upload_to_bigquery(table_ref, job_listing)\n",
        "                time.sleep(4)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting job details: {e}\")\n",
        "                continue\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "def upload_to_bigquery(table_ref, job_listing):\n",
        "    \"\"\"Uploads a single job listing to a BigQuery table.\"\"\"\n",
        "    client = bigquery.Client()\n",
        "\n",
        "    # Add the current timestamp to the job listing\n",
        "    job_listing[\"inserted_at\"] = datetime.utcnow().isoformat()  # Add UTC timestamp\n",
        "\n",
        "    # Insert single job listing into the table\n",
        "    errors = client.insert_rows_json(table_ref, [job_listing])  # Insert a single job listing\n",
        "    if errors == []:\n",
        "        print(f\"Job successfully inserted into {table_ref.table_id}.\")\n",
        "    else:\n",
        "        print(f\"Errors occurred while inserting job: {errors}\")\n",
        "\n",
        "def ensure_table_exists(client, project_id, dataset_id, table_id):\n",
        "    \"\"\"Ensures the BigQuery table exists. Creates it if it doesn't.\"\"\"\n",
        "    table_ref = client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"job_title\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"company\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"summary\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"scraped_occupation_title\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"inserted_at\", \"TIMESTAMP\")\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        table = client.get_table(table_ref)\n",
        "        print(f\"Table {table_id} already exists.\")\n",
        "    except Exception as e:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        client.create_table(table)\n",
        "        print(f\"Created table {table_id}.\")\n",
        "\n",
        "    return table_ref\n",
        "\n",
        "# List of occupations to scrape\n",
        "occupations = [\"Warehouse Associate\"]  # done with energy analyst, product manager, HVAC Engineer, Nurse, and welder\n",
        "\n",
        "# BigQuery details\n",
        "project_id = \"intern-sandbox-427618\"\n",
        "dataset_id = \"apprenticeship_data\"\n",
        "table_id = \"apprenticeship_jobs_data\"\n",
        "\n",
        "# Initialize BigQuery client and ensure table exists once\n",
        "client = bigquery.Client(project=project_id)\n",
        "table_ref = ensure_table_exists(client, project_id, dataset_id, table_id)\n",
        "\n",
        "# Iterate over each occupation and scrape job listings\n",
        "for occupation in occupations:\n",
        "    scrape_jobs(occupation, project_id, dataset_id, table_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5OjQVILId8S4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "executionInfo": {
          "elapsed": 12759,
          "status": "error",
          "timestamp": 1726083146832,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "5OjQVILId8S4",
        "outputId": "1c545402-359d-4fc8-8690-16c1d5e629dd"
      },
      "outputs": [],
      "source": [
        "#this works with bigquery, the data is scraped data first and then put into bigquery all together\n",
        "from google.cloud import bigquery\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "from selenium import webdriver\n",
        "import google_colab_selenium as gs\n",
        "\n",
        "def scrape_jobs(occupation):\n",
        "    # Initialize WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode\n",
        "    options.add_argument('--disable-blink-features=AutomationControlled')  # Make Selenium less detectable\n",
        "    driver = gs.Chrome(options=options)\n",
        "\n",
        "    url = f\"https://www.apprenticeship.gov/finder/listings?occupation={occupation.replace(' ', '%20')}&location=\"\n",
        "    driver.get(url)\n",
        "\n",
        "    # List to store the scraped data\n",
        "    job_listings = []\n",
        "\n",
        "    try:\n",
        "        # Load More button logic\n",
        "        while True:\n",
        "            try:\n",
        "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # Scroll to the bottom of the page\n",
        "                load_more_button = WebDriverWait(driver, 10).until(\n",
        "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \".listings-cards-load-more-btn-container .btn.btn--secondary.load-more\"))\n",
        "                )\n",
        "                load_more_button.click()\n",
        "                time.sleep(2)  # Wait for new jobs to load\n",
        "            except Exception as e:\n",
        "                print(f\"Load more button not found or not clickable: {e}\")\n",
        "                break\n",
        "\n",
        "        # Wait for job listings to load\n",
        "        jobs = WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]'))\n",
        "        )\n",
        "\n",
        "        # Iterate over each job listing\n",
        "        for i in range(len(jobs)):\n",
        "            try:\n",
        "                # Re-find job elements to avoid stale element reference\n",
        "                jobs = driver.find_elements(By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]')\n",
        "                job = jobs[i]\n",
        "                time.sleep(2)\n",
        "\n",
        "                # Extract title, program, and location\n",
        "                title = job.find_element(By.CLASS_NAME, 'left-section_main-title').text\n",
        "                program = job.find_element(By.CLASS_NAME, 'bottom-content_p-title').text\n",
        "                location = job.find_element(By.CLASS_NAME, 'bottom-content_p-content').text\n",
        "\n",
        "                # Click the job to open the details view\n",
        "                job.click()\n",
        "\n",
        "                # Wait for the job summary to load\n",
        "                summary_element = WebDriverWait(driver, 10).until(\n",
        "                    EC.presence_of_element_located((By.CLASS_NAME, \"description-section\"))\n",
        "                )\n",
        "                summary = summary_element.text\n",
        "\n",
        "                # Append the job details to the list\n",
        "                job_listings.append({\n",
        "                    \"job_title\": title,\n",
        "                    \"company\": program,\n",
        "                    \"location\": location,\n",
        "                    \"summary\": summary,\n",
        "                    \"scraped_occupation_title\": occupation\n",
        "                })\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting job details: {e}\")\n",
        "                continue\n",
        "    finally:\n",
        "        driver.quit()\n",
        "\n",
        "    return job_listings\n",
        "\n",
        "def upload_to_bigquery(project_id, dataset_id, table_id, job_listings):\n",
        "    \"\"\"Uploads data to a BigQuery table.\"\"\"\n",
        "    client = bigquery.Client(project=project_id)\n",
        "\n",
        "    # Define table schema\n",
        "    schema = [\n",
        "        bigquery.SchemaField(\"job_title\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"company\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"location\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"summary\", \"STRING\"),\n",
        "        bigquery.SchemaField(\"scraped_occupation_title\", \"STRING\")\n",
        "    ]\n",
        "\n",
        "    # Create the table if it doesn't exist\n",
        "    table_ref = client.dataset(dataset_id).table(table_id)\n",
        "    try:\n",
        "        table = client.get_table(table_ref)\n",
        "        print(f\"Table {table_id} already exists.\")\n",
        "    except Exception as e:\n",
        "        table = bigquery.Table(table_ref, schema=schema)\n",
        "        table = client.create_table(table)\n",
        "        print(f\"Created table {table_id}.\")\n",
        "\n",
        "    # Insert data into the table\n",
        "    errors = client.insert_rows_json(table_ref, job_listings)\n",
        "    if errors == []:\n",
        "        print(f\"Data successfully inserted into {table_id}.\")\n",
        "    else:\n",
        "        print(f\"Errors occurred while inserting data: {errors}\")\n",
        "\n",
        "# List of occupations to scrape\n",
        "occupations = [\"Product Manager\"]\n",
        "\n",
        "# BigQuery details\n",
        "project_id = \"intern-sandbox-427618\"\n",
        "dataset_id = \"apprenticeship_data\"\n",
        "table_id = \"apprenticeship_jobs_data\"\n",
        "\n",
        "# Iterate over each occupation and scrape job listings\n",
        "for occupation in occupations:\n",
        "    job_listings = scrape_jobs(occupation)\n",
        "    upload_to_bigquery(project_id, dataset_id, table_id, job_listings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brPWsGCmuOnp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "executionInfo": {
          "elapsed": 219,
          "status": "error",
          "timestamp": 1726159634613,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "brPWsGCmuOnp",
        "outputId": "0816cdae-6b4f-4c4f-8708-02dd13b234f5"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Replace with your Google Cloud project ID and dataset ID\n",
        "project_id = \"intern-sandbox-427618\"\n",
        "dataset_id = \"apprenticeship_data\"\n",
        "\n",
        "# Initialize a BigQuery client\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Define the table ID\n",
        "table_id = f\"{project_id}.{dataset_id}.apprenticeship_jobs_data\"\n",
        "\n",
        "# Define the schema for the table\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"job_title\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"company\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"location\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"summary\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"scraped_occupation_title\", \"STRING\")\n",
        "]\n",
        "\n",
        "\n",
        "# Create the table object\n",
        "table = bigquery.Table(table_id, schema=schema)\n",
        "\n",
        "# Create the table in BigQuery\n",
        "table = client.create_table(table)  # API request\n",
        "print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yvDXVFfgIvuD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 755,
          "status": "ok",
          "timestamp": 1726159718584,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "yvDXVFfgIvuD",
        "outputId": "c65cbac2-3a51-403e-be1b-4a30f5a2d45b"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "# Replace with your Google Cloud project ID and dataset ID\n",
        "project_id = \"intern-sandbox-427618\"\n",
        "dataset_id = \"apprenticeship_data\"\n",
        "table_id = f\"{project_id}.{dataset_id}.apprenticeship_jobs_data\"\n",
        "\n",
        "# Initialize a BigQuery client\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Get the current table object\n",
        "table = client.get_table(table_id)\n",
        "\n",
        "# Define the new schema field\n",
        "new_schema_field = bigquery.SchemaField(\"inserted_at\", \"TIMESTAMP\")\n",
        "\n",
        "# Add the new field to the existing schema\n",
        "new_schema = list(table.schema)  # Copy the existing schema\n",
        "new_schema.append(new_schema_field)  # Append the new column\n",
        "\n",
        "# Update the table object with the new schema\n",
        "table.schema = new_schema\n",
        "\n",
        "# Update the table in BigQuery\n",
        "table = client.update_table(table, [\"schema\"])  # API request\n",
        "\n",
        "# Print confirmation\n",
        "print(f\"Table {table.project}.{table.dataset_id}.{table.table_id} schema updated successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5r9PnZdj1Yit",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "elapsed": 1325029,
          "status": "ok",
          "timestamp": 1723738058116,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 240
        },
        "id": "5r9PnZdj1Yit",
        "outputId": "28b8fb4c-0444-4fa4-d556-ab8f40420c80"
      },
      "outputs": [],
      "source": [
        "#Without bigquery and with bucket storage\n",
        "\n",
        "from google.cloud import storage\n",
        "import csv\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import google_colab_selenium as gs\n",
        "\n",
        "\n",
        "def scrape_jobs(occupation):\n",
        "    # Initialize WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode\n",
        "    # driver = webdriver.Chrome(options=options)\n",
        "    options.add_argument('--disable-blink-features=AutomationControlled')  # Make Selenium less detectable\n",
        "    driver = gs.Chrome(options=options)\n",
        "\n",
        "    url = f\"https://www.apprenticeship.gov/finder/listings?occupation={occupation.replace(' ', '%20')}&location=\"\n",
        "    driver.get(url)\n",
        "\n",
        "    # CSV file setup\n",
        "    source_file_name = f'job_listings_{occupation.replace(\" \", \"_\")}.csv'\n",
        "    with open(source_file_name, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        # Write the header row\n",
        "        writer.writerow([\"Title\", \"Job\", \"Location\", \"Summary\", \"scraped_occupation_title\"])\n",
        "\n",
        "        try:\n",
        "            # Load More button logic\n",
        "            while True:\n",
        "                try:\n",
        "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")  # Scroll to the bottom of the page\n",
        "                    load_more_button = WebDriverWait(driver, 10).until(\n",
        "                        EC.element_to_be_clickable((By.CSS_SELECTOR, \".listings-cards-load-more-btn-container .btn.btn--secondary.load-more\"))\n",
        "                    )\n",
        "                    load_more_button.click()\n",
        "                    time.sleep(2)  # Wait for new jobs to load\n",
        "                except Exception as e:\n",
        "                    print(f\"Load more button not found or not clickable: {e}\")\n",
        "                    break\n",
        "\n",
        "            # Wait for job listings to load\n",
        "            jobs = WebDriverWait(driver, 20).until(\n",
        "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]'))\n",
        "            )\n",
        "\n",
        "            # Iterate over each job listing\n",
        "            for i in range(len(jobs)):\n",
        "                try:\n",
        "                    # Re-find job elements to avoid stale element reference\n",
        "                    jobs = driver.find_elements(By.CSS_SELECTOR, '[data-test=\"component-listing-card\"]')\n",
        "                    job = jobs[i]\n",
        "                    time.sleep(2)\n",
        "\n",
        "                    # Extract title, program, and location\n",
        "                    title = job.find_element(By.CLASS_NAME, 'left-section_main-title').text\n",
        "                    program = job.find_element(By.CLASS_NAME, 'bottom-content_p-title').text\n",
        "                    location = job.find_element(By.CLASS_NAME, 'bottom-content_p-content').text\n",
        "\n",
        "                    # Click the job to open the details view\n",
        "                    job.click()\n",
        "\n",
        "                    # Wait for the job summary to load\n",
        "                    summary_element = WebDriverWait(driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.CLASS_NAME, \"description-section\"))\n",
        "                    )\n",
        "                    summary = summary_element.text\n",
        "\n",
        "                    # Write to the CSV file\n",
        "                    writer.writerow([title, program, location, summary, occupation])\n",
        "                    time.sleep(1)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting job details: {e}\")\n",
        "                    continue\n",
        "        finally:\n",
        "            driver.quit()\n",
        "\n",
        "    return source_file_name\n",
        "\n",
        "def upload_to_gcp_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the GCP bucket.\"\"\"\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# List of occupations to scrape\n",
        "occupations = [\"Energy Analyst\"]\n",
        "\n",
        "# GCP bucket name\n",
        "bucket_name = \"scrapingdata-harshit\"\n",
        "\n",
        "# Iterate over each occupation and scrape job listings\n",
        "for occupation in occupations:\n",
        "    source_file_name = scrape_jobs(occupation)\n",
        "    destination_blob_name = f\"RAPIDS-Jobs-Data/job_listings_{occupation.replace(' ', '_')}.csv\"\n",
        "    upload_to_gcp_bucket(bucket_name, source_file_name, destination_blob_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "apprenticeship-job-scraper",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
